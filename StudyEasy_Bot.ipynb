{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfplumber PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "# !pip install --quiet  nltk faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install emoji\n",
    "# !pip install nltk\n",
    "# !pip install pdfplumber\n",
    "# !pip install PyPDF2\n",
    "# !pip install sentence-transformers\n",
    "# !pip install faiss-cpu  # Ø§Ø³ØªØ®Ø¯Ù… faiss-gpu Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ GPU Ù…Ø¯Ø¹ÙˆÙ…\n",
    "# !pip install python-telegram-bot==20.0  # Ø£Ùˆ Ù†Ø³Ø®Ø© Ù‚Ø±ÙŠØ¨Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ù„Ù€ ApplicationBuilder\n",
    "# !pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ø§Ù„ÙˆØ§Ø±Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ===\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pdfplumber\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import asyncio\n",
    "\n",
    "# from telegram import ReplyKeyboardMarkup # Import ReplyKeyboardMarkup from telegram\n",
    "# from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters, Update # Import Update from telegram.ext\n",
    "\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "from telegram import Update\n",
    "\n",
    "import os\n",
    "import nest_asyncio\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "# === ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ¦Ø© ===\n",
    "nest_asyncio.apply()\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW8kN8V5wXoY"
   },
   "source": [
    "# Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGStore:\n",
    "    def __init__(self):\n",
    "        self.chunks: List[str] = []\n",
    "        self.metas: List[Dict] = []\n",
    "        self.index = None\n",
    "        self.emb_dim = None\n",
    "\n",
    "    def add_text(self, text: str, source: str = \"user\") -> None:\n",
    "        \"\"\"Ø¥Ø¶Ø§ÙØ© Ù†Øµ Ø¬Ø¯ÙŠØ¯ Ø¥Ù„Ù‰ Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¹Ø±ÙØ©\"\"\"\n",
    "        chunks = self._chunk_text(text)\n",
    "        embs = embedder.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "        if self.index is None:\n",
    "            self.emb_dim = embs.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(self.emb_dim)\n",
    "        self.index.add(embs)\n",
    "\n",
    "        start_id = len(self.chunks)\n",
    "        self.chunks.extend(chunks)\n",
    "        for i in range(len(chunks)):\n",
    "            self.metas.append({\"source\": source, \"chunk_id\": start_id + i})\n",
    "\n",
    "    @staticmethod\n",
    "    def _chunk_text(text: str, chunk_words: int = 400) -> List[str]:\n",
    "        \"\"\"ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£ØµØºØ±\"\"\"\n",
    "        words = text.split()\n",
    "        return [' '.join(words[i:i+chunk_words]) for i in range(0, len(words), chunk_words)]\n",
    "\n",
    "    def search(self, query: str, top_k: int = 4) -> List[Tuple[str, Dict, float]]:\n",
    "        \"\"\"Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨\"\"\"\n",
    "        if not self.chunks:\n",
    "            return []\n",
    "\n",
    "        q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "        D, I = self.index.search(q_emb, min(top_k, len(self.chunks)))\n",
    "\n",
    "        return [\n",
    "            (self.chunks[idx], self.metas[idx], float(score))\n",
    "            for score, idx in zip(D[0], I[0])\n",
    "            if idx != -1\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMpevAhdN2uh"
   },
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e386cd55cf46cb8f47455cde2a3cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfce0d36de047418e8ea248b02a6817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e63f8b28d84725a5139caaad49e6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c986de5cbbe4b4c8978ecdfa34c3ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d65abf79034a92b36a82548524b38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dba30035d434476ab10afc7ccebf4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9640722ed0fe45cf976e2a2f4fe06a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30697c5cac574961b04cfc9e0b6f4dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e83d164f924636af5d9793d0a833e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4927185151af4544ae894f09c927bc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7834cc25e9714558a71a19a7e7ec7d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f6b91fc1404b8d83320e0a0a16322a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5709b844ad3e4e56a9020878cc2d1a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# === Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ===\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "GEN_MODEL = \"google/flan-t5-large\"\n",
    "EMBED_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL).to(\"cuda\" if device == 0 else \"cpu\")\n",
    "\n",
    "# âœ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù€ pipeline (ÙƒØ§Ù†Øª Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©!)\n",
    "text2text_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¶Ù…ÙŠÙ†\n",
    "embedder = SentenceTransformer(EMBED_MODEL, device=\"cuda\" if device == 0 else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {model.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_FcrsD0O2pJ"
   },
   "source": [
    "#  Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str, remove_stopwords: bool = True) -> Tuple[str, int]:\n",
    "    \"\"\"Clean text from unwanted characters and emojis\"\"\"\n",
    "    emoji_count = sum(1 for c in text if c in emoji.EMOJI_DATA)\n",
    "    text = emoji.replace_emoji(text, replace=' ')\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    lines = [line.strip() for line in text.splitlines() if len(line.strip()) > 5]\n",
    "    text = ' '.join(lines)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        text = ' '.join(w for w in text.split() if w.lower() not in STOPWORDS)\n",
    "\n",
    "    return re.sub(r'\\s+', ' ', text).strip(), emoji_count\n",
    "\n",
    "async def process_with_typing_indicator(update: Update, context: ContextTypes.DEFAULT_TYPE, task, *args):\n",
    "    \"\"\"Helper function to show typing indicator during processing\"\"\"\n",
    "    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=\"typing\")\n",
    "    try:\n",
    "        return await task(*args)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {e}\")\n",
    "        await update.message.reply_text(\"âŒ An error occurred during processing. Please try again.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRRbyFaLz5DB"
   },
   "source": [
    "# generate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def generate_summary(text: str) -> str:\n",
    "    \"\"\"Generate a concise summary in English\"\"\"\n",
    "    chunks = RAGStore._chunk_text(text, 800)\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = f\"Summarize the following text in 3-4 clear points:\\n\\n{chunk}\"\n",
    "        out = await asyncio.to_thread(\n",
    "            text2text_pipe,\n",
    "            prompt,\n",
    "            max_length=200,\n",
    "            min_length=60,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        if out and len(out) > 0:\n",
    "            summary = out[0]['generated_text'].strip()\n",
    "            if summary:\n",
    "                summaries.append(summary)\n",
    "\n",
    "    final = \"\\n\\n\".join(summaries)\n",
    "    return final if final.strip() else \"No summary could be generated.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owc4IdOszybp"
   },
   "source": [
    "# generate simplified explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def generate_simplified_explanation(text: str) -> str:\n",
    "    \"\"\"Generate a simplified explanation in plain English\"\"\"\n",
    "    chunks = RAGStore._chunk_text(text, 700)\n",
    "    explanations = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = f\"Explain this text in simple English for a high school student. Use short sentences and clear language:\\n\\n{chunk}\"\n",
    "        out = await asyncio.to_thread(\n",
    "            text2text_pipe,\n",
    "            prompt,\n",
    "            max_length=250,\n",
    "            min_length=80,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        if out and len(out) > 0:\n",
    "            explanation = out[0]['generated_text'].strip()\n",
    "            if explanation:\n",
    "                explanations.append(explanation)\n",
    "\n",
    "    final = \"\\n\\n\".join(explanations)\n",
    "    return final if final.strip() else \"No explanation could be generated.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxeXDQflzwfh"
   },
   "source": [
    "# generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def generate_answer(question: str, contexts: List[str]) -> str:\n",
    "    \"\"\"Answer a question in English based on context\"\"\"\n",
    "    context_text = \"\\n\".join(contexts[:3])\n",
    "    prompt = f\"Answer the question based on the context below. If the answer is not in the context, say 'I don't know'.\\n\\nContext:\\n{context_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    out = await asyncio.to_thread(\n",
    "        text2text_pipe,\n",
    "        prompt,\n",
    "        max_length=300,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        num_beams=3\n",
    "    )\n",
    "\n",
    "    if out and len(out) > 0:\n",
    "        answer = out[0]['generated_text'].strip()\n",
    "        return answer if answer else \"I don't know.\"\n",
    "    return \"I don't know.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EBiCcJoznJh"
   },
   "source": [
    "#  Read Pdf content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_pdf_content(file_bytes: bytes) -> str:\n",
    "    \"\"\"Read PDF content using multiple libraries for reliability\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                extracted = page.extract_text()\n",
    "                if extracted:\n",
    "                    text += extracted + \"\\n\"\n",
    "        if text.strip():\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"pdfplumber error: {e}\")\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(io.BytesIO(file_bytes))\n",
    "        for page in reader.pages:\n",
    "            extracted = page.extract_text()\n",
    "            if extracted:\n",
    "                text += extracted + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF2 error: {e}\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea87gRsajFum"
   },
   "source": [
    "#bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ===\n",
    "MENU_KEYBOARD = ReplyKeyboardMarkup(\n",
    "    [\n",
    "        [\"Summarize Text ðŸ“\", \"Explain Text ðŸ“–\"],\n",
    "        [\"Ask Questions â“\", \"Clear Memory ðŸ—‘ï¸\"],\n",
    "        [\"Help â„¹ï¸\"]\n",
    "    ],\n",
    "    resize_keyboard=True,\n",
    "    one_time_keyboard=False\n",
    ")\n",
    "\n",
    "user_data = {}\n",
    "\n",
    "# === Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø£ÙˆØ§Ù…Ø± ===\n",
    "async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    user_id = update.message.from_user.id\n",
    "    user_data[user_id] = {\"rag\": RAGStore(), \"mode\": None}\n",
    "    await update.message.reply_text(\n",
    "        \"ðŸ‘‹ Welcome to the Study Assistant Bot!\\n\\n\"\n",
    "        \"I can help you with:\\n\"\n",
    "        \"â€¢ ðŸ“ Summarizing texts and PDFs\\n\"\n",
    "        \"â€¢ ðŸ“– Explaining content in simple English\\n\"\n",
    "        \"â€¢ â“ Answering your questions about the material\\n\\n\"\n",
    "        \"Send me a text or PDF to get started!\",\n",
    "        reply_markup=MENU_KEYBOARD\n",
    "    )\n",
    "\n",
    "async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    await update.message.reply_text(\n",
    "        \"ðŸ†˜ Help:\\n\\n\"\n",
    "        \"â€¢ Send a PDF or text\\n\"\n",
    "        \"â€¢ Use the menu to Summarize, Explain, or Ask\\n\"\n",
    "        \"â€¢ Type 'exit' to leave question mode\",\n",
    "        reply_markup=MENU_KEYBOARD\n",
    "    )\n",
    "\n",
    "async def handle_menu_selection(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    user_id = update.message.from_user.id\n",
    "    if user_id not in user_data:\n",
    "        await start_command(update, context)\n",
    "        return\n",
    "\n",
    "    choice = update.message.text\n",
    "    rag_store = user_data[user_id][\"rag\"]\n",
    "\n",
    "    if \"Summarize\" in choice:\n",
    "        if not rag_store.chunks:\n",
    "            await update.message.reply_text(\"âš ï¸ No content loaded yet. Please send a PDF or text first.\")\n",
    "            return\n",
    "        await update.message.reply_text(\"â³ Generating summary...\")\n",
    "        summary = await process_with_typing_indicator(update, context, generate_summary, \" \".join(rag_store.chunks))\n",
    "        if summary:\n",
    "            await update.message.reply_text(f\"ðŸ“ Summary:\\n\\n{summary}\\n\\nChoose another option ðŸ‘‡\", reply_markup=MENU_KEYBOARD)\n",
    "\n",
    "    elif \"Explain\" in choice:\n",
    "        if not rag_store.chunks:\n",
    "            await update.message.reply_text(\"âš ï¸ No content loaded yet. Please send a PDF or text first.\")\n",
    "            return\n",
    "        await update.message.reply_text(\"â³ Generating explanation...\")\n",
    "        explanation = await process_with_typing_indicator(update, context, generate_simplified_explanation, \" \".join(rag_store.chunks))\n",
    "        if explanation:\n",
    "            await update.message.reply_text(f\"ðŸ“– Explanation:\\n\\n{explanation}\\n\\nChoose another option ðŸ‘‡\", reply_markup=MENU_KEYBOARD)\n",
    "\n",
    "    elif \"Ask\" in choice:\n",
    "        user_data[user_id][\"mode\"] = \"qa\"\n",
    "        await update.message.reply_text(\n",
    "            \"ðŸ’¬ Ask any question about the content.\\nType 'exit' to return.\",\n",
    "            reply_markup=ReplyKeyboardMarkup([[\"exit\"]], resize_keyboard=True)\n",
    "        )\n",
    "\n",
    "    elif \"Clear\" in choice:\n",
    "        user_data[user_id] = {\"rag\": RAGStore(), \"mode\": None}\n",
    "        await update.message.reply_text(\"ðŸ§¹ Memory cleared. Send a new document.\", reply_markup=MENU_KEYBOARD)\n",
    "\n",
    "    elif \"Help\" in choice:\n",
    "        await help_command(update, context)\n",
    "\n",
    "async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    user_id = update.message.from_user.id\n",
    "    if user_id not in user_data:\n",
    "        user_data[user_id] = {\"rag\": RAGStore(), \"mode\": None}\n",
    "\n",
    "    await update.message.reply_text(\"â³ Processing your PDF...\")\n",
    "    try:\n",
    "        file = await update.message.document.get_file()\n",
    "        file_bytes = await file.download_as_bytearray()\n",
    "        text = read_pdf_content(file_bytes)\n",
    "\n",
    "        if not text.strip():\n",
    "            await update.message.reply_text(\"âš ï¸ Could not read text from PDF.\")\n",
    "            return\n",
    "\n",
    "        clean_text_content, _ = clean_text(text)\n",
    "        user_data[user_id][\"rag\"].add_text(clean_text_content, source=\"document\")\n",
    "\n",
    "        await update.message.reply_text(\n",
    "            \"âœ… PDF processed!\\nNow choose: Summarize, Explain, or Ask Questions\",\n",
    "            reply_markup=MENU_KEYBOARD\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"PDF Error: {e}\")\n",
    "        await update.message.reply_text(\"âŒ Failed to process PDF.\")\n",
    "\n",
    "async def handle_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    user_id = update.message.from_user.id\n",
    "    if user_id not in user_data:\n",
    "        await start_command(update, context)\n",
    "        return\n",
    "\n",
    "    text = update.message.text.strip()\n",
    "\n",
    "    if text.lower() in [\"exit\", \"quit\"]:\n",
    "        user_data[user_id][\"mode\"] = None\n",
    "        await update.message.reply_text(\"Back to main menu.\", reply_markup=MENU_KEYBOARD)\n",
    "        return\n",
    "\n",
    "    if user_data[user_id].get(\"mode\") == \"qa\":\n",
    "        rag_store = user_data[user_id][\"rag\"]\n",
    "        if not rag_store.chunks:\n",
    "            await update.message.reply_text(\"âš ï¸ No content to answer questions.\")\n",
    "            return\n",
    "        await update.message.reply_text(\"ðŸ” Finding answer...\")\n",
    "        results = rag_store.search(text, top_k=3)\n",
    "        contexts = [res[0] for res in results]\n",
    "        answer = await process_with_typing_indicator(update, context, generate_answer, text, contexts)\n",
    "        await update.message.reply_text(f\"â“ {text}\\n\\nðŸ’¡ {answer}\\n\\nAsk more or type 'exit'.\")\n",
    "    else:\n",
    "        clean_text_content, _ = clean_text(text)\n",
    "        user_data[user_id][\"rag\"].add_text(clean_text_content, source=\"text\")\n",
    "        await update.message.reply_text(\n",
    "            \"âœ… Text saved!\\nNow choose: Summarize, Explain, or Ask Questions\",\n",
    "            reply_markup=MENU_KEYBOARD\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª ===\n",
    "def setup_application() -> ApplicationBuilder:\n",
    "    app = ApplicationBuilder().token(\"8349476459:AAGX8-RT7oAUvZo_Ln1Z2RTAt93t3pjBWZU\").build()\n",
    "    app.add_handler(CommandHandler(\"start\", start_command))\n",
    "    app.add_handler(CommandHandler(\"help\", help_command))\n",
    "    app.add_handler(MessageHandler(filters.Regex(\"^(Summarize|Explain|Ask|Clear|Help)\"), handle_menu_selection))\n",
    "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text_message))\n",
    "    app.add_handler(MessageHandler(filters.Document.PDF, handle_document))\n",
    "    return app\n",
    "\n",
    "async def run_bot():\n",
    "    app = setup_application()\n",
    "    await app.run_polling()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_bot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
